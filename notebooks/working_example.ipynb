{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0531c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "import collections\n",
    "from functools import partial\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from numpy.random import uniform\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "import colorlog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fb9c650",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gnn_tracking.utils.plotting import EventPlotter\n",
    "\n",
    "n_evts, n_sectors = 20, 32\n",
    "savefig = False\n",
    "indir='/tigress/jdezoort/codalab/train_1'\n",
    "event_plotter = EventPlotter(indir=indir)\n",
    "event_plotter.plot_ep_rv_uv(evtid=21289, savefig=savefig,\n",
    "                            filename='../plots/full_event.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cde54fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gnn_tracking.preprocessing.point_cloud_builder import PointCloudBuilder\n",
    "\n",
    "# build point clouds for each sector in the pixel layers only\n",
    "pc_builder = PointCloudBuilder(indir='/tigress/jdezoort/codalab/train_1',\n",
    "                               outdir='../point_clouds/',\n",
    "                               n_sectors=n_sectors, pixel_only=True, \n",
    "                               redo=False, measurement_mode=False,\n",
    "                               sector_di=0, sector_ds=1.3, thld=0.9,\n",
    "                               log_level=0)\n",
    "pc_builder.process(n=n_evts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2020ca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# each point cloud is a PyG Data object \n",
    "point_cloud = pc_builder.data_list\n",
    "pc_builder.get_measurements()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11dc520f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gnn_tracking.utils.plotting import PointCloudPlotter\n",
    "\n",
    "# visualize the sectors in each event and an overlapped ('extended') sector\n",
    "pc_plotter = PointCloudPlotter('../point_clouds', \n",
    "                               n_sectors=pc_builder.n_sectors)\n",
    "pc_plotter.plot_ep_rv_uv_all_sectors(21289, savefig=savefig, filename='../plots/point_cloud.pdf')\n",
    "pc_plotter.plot_ep_rv_uv_with_boundary(21289, 18, \n",
    "                                       pc_builder.sector_di,\n",
    "                                       pc_builder.sector_ds,\n",
    "                                       savefig=savefig, \n",
    "                                       filename='../plots/point_cloud_extended.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91897cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can build graphs on the point clouds using geometric cuts\n",
    "from gnn_tracking.graph_construction.graph_builder import GraphBuilder\n",
    "\n",
    "graph_builder = GraphBuilder(indir='../point_clouds/for_paper', outdir='../graphs/for_paper', \n",
    "                             redo=False, measurement_mode=False, \n",
    "                             phi_slope_max=0.0035, z0_max=200, dR_max=2.3,\n",
    "                             log_level=1)\n",
    "graph_builder.process(n=1600)\n",
    "graph_builder.get_measurements()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aeefbe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gnn_tracking.utils.plotting import GraphPlotter\n",
    "\n",
    "# the graph plotter shows the true and false edges constructed by the builder\n",
    "graph_plotter = GraphPlotter(indir='../graphs/')\n",
    "graph = graph_builder.data_list[0]\n",
    "print(graph)\n",
    "evtid, s = graph.evtid.item(), graph.s.item()\n",
    "\n",
    "#graph_plotter.plot_rz(graph_builder.data_list[0], \n",
    "#                      f'event{evtid}_s{s}', \n",
    "#                      scale=np.array([1,1,1]))\n",
    "\n",
    "graph_plotter.plot_ep_rz_uv(graph, sector=s, name=f'data{evtid}_s{s}',\n",
    "                            savefig=savefig, filename='../plots/graphs.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c987bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gnn_tracking.models.track_condensation_networks import GraphTCN\n",
    "from gnn_tracking.training.tcn_trainer import TCNTrainer\n",
    "from gnn_tracking.utils.losses import (\n",
    "    EdgeWeightBCELoss,\n",
    "    EdgeWeightFocalLoss,\n",
    "    PotentialLoss,\n",
    "    BackgroundLoss,\n",
    "    ObjectLoss,\n",
    ")\n",
    "\n",
    "# use cuda (gpu) if possible, otherwise fallback to cpu\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "print(f'Utilizing {device}')\n",
    "\n",
    "# use reference graph to get relevant dimensions \n",
    "g = graph_builder.data_list[0]\n",
    "node_indim = g.x.shape[1]\n",
    "edge_indim = g.edge_attr.shape[1]\n",
    "\n",
    "# partition graphs into train, test, val splits\n",
    "graphs = graph_builder.data_list\n",
    "n_graphs = len(graphs)\n",
    "rand_array = uniform(low=0, high=1, size=n_graphs)\n",
    "train_graphs = [g for i, g in enumerate(graphs) if (rand_array<=0.75)[i]]\n",
    "test_graphs = [g for i, g in enumerate(graphs) if ((rand_array>0.75) & (rand_array<=0.95))[i]]\n",
    "val_graphs = [g for i, g in enumerate(graphs) if (rand_array>0.95)[i]]\n",
    "\n",
    "# build graph loaders\n",
    "params = {'batch_size': 1, 'shuffle': True, 'num_workers': 2}\n",
    "train_loader = DataLoader(list(train_graphs), **params)\n",
    "params = {'batch_size': 1, 'shuffle': False, 'num_workers': 2}\n",
    "test_loader = DataLoader(list(test_graphs), **params)\n",
    "val_loader = DataLoader(list(val_graphs), **params)\n",
    "loaders = {'train': train_loader, 'test': test_loader,\n",
    "           'val': val_loader}\n",
    "print('Loader sizes:', [(k, len(v)) for k, v in loaders.items()])\n",
    "\n",
    "# build loss function dictionary\n",
    "q_min, sb = 0.01, 1\n",
    "loss_functions = {\n",
    "    \"edge\": EdgeWeightFocalLoss(gamma=5, alpha=0.95).to(device),\n",
    "    \"potential\": PotentialLoss(q_min=q_min, device=device),\n",
    "    \"background\": BackgroundLoss(device=device, sb=sb),\n",
    "    #\"object\": ObjectLoss(device=device, mode='efficiency')\n",
    "}\n",
    "\n",
    "loss_weights = {\n",
    "    # everything that's not mentioned here will be 1\n",
    "    \"edge\": 500,\n",
    "    \"potential_attractive\": 500,\n",
    "    \"potential_repulsive\": 5,\n",
    "    \"background\": 0.05,\n",
    "    #\"object\": 1/250000,\n",
    "}\n",
    "\n",
    "# set up a model and trainer\n",
    "model = GraphTCN(\n",
    "    node_indim, \n",
    "    edge_indim,\n",
    "    h_dim=10,\n",
    "    e_dim=10,\n",
    "    L_ec=5,\n",
    "    L_hc=2,\n",
    "    h_outdim=10, # output dim of the latent space\n",
    "    hidden_dim=128,\n",
    ")\n",
    "model_parameters = filter(lambda p: p.requires_grad, \n",
    "                            model.parameters())\n",
    "n_params = sum([np.prod(p.size()) for p in model_parameters])\n",
    "print('number trainable params:', n_params)\n",
    "\n",
    "scheduler = partial(StepLR, gamma=0.95, step_size=4)\n",
    "trainer = TCNTrainer(\n",
    "    model=model,\n",
    "    loaders=loaders, \n",
    "    loss_functions=loss_functions,\n",
    "    lr=0.005, \n",
    "    loss_weights=loss_weights, \n",
    "    device=device,\n",
    "    lr_scheduler=scheduler,\n",
    ")\n",
    "print(trainer.loss_functions)\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52626032",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import StepLR \n",
    "from gnn_tracking.models.track_condensation_networks import PointCloudTCN\n",
    "from gnn_tracking.training.tcn_trainer import TCNTrainer\n",
    "from gnn_tracking.utils.losses import (\n",
    "    EdgeWeightBCELoss,\n",
    "    PotentialLoss,\n",
    "    BackgroundLoss,\n",
    "    ObjectLoss,\n",
    ")\n",
    "\n",
    "# use cuda (gpu) if possible, otherwise fallback to cpu\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "print(f'Utilizing {device}')\n",
    "\n",
    "# use reference graph to get relevant dimensions \n",
    "p = pc_builder.data_list[0]\n",
    "node_indim = p.x.shape[1]\n",
    "hc_outdim = 2 # output dim of latent space  \n",
    "\n",
    "# partition graphs into train, test, val splits\n",
    "point_clouds = pc_builder.data_list\n",
    "n_pcs = len(point_clouds)\n",
    "rand_array = uniform(low=0, high=1, size=n_pcs)\n",
    "train_pcs = [p for i, p in enumerate(point_clouds) if (rand_array<=0.6)[i]]\n",
    "test_pcs = [p for i, p in enumerate(point_clouds) if ((rand_array>0.6) & (rand_array<=0.8))[i]]\n",
    "val_pcs = [p for i, p in enumerate(point_clouds) if (rand_array>0.8)[i]]\n",
    "\n",
    "# build graph loaders\n",
    "params = {'batch_size': 1, 'shuffle': True, 'num_workers': 2}\n",
    "train_loader = DataLoader(list(train_pcs), **params)\n",
    "params = {'batch_size': 1, 'shuffle': False, 'num_workers': 2}\n",
    "test_loader = DataLoader(list(test_pcs), **params)\n",
    "val_loader = DataLoader(list(val_pcs), **params)\n",
    "loaders = {'train': train_loader, 'test': test_loader,\n",
    "           'val': val_loader}\n",
    "print('Loader sizes:', [(k, len(v)) for k, v in loaders.items()])\n",
    "\n",
    "# build loss function dictionary\n",
    "q_min, sb = 0.01, 0.1\n",
    "loss_functions = {\n",
    "    \"potential\": PotentialLoss(q_min=q_min, device=device),\n",
    "    \"background\": BackgroundLoss(device=device, sb=sb),\n",
    "    #\"object\": ObjectLoss(device=device, mode='efficiency')\n",
    "}\n",
    "\n",
    "loss_weights = {\n",
    "    # everything that's not mentioned here will be 1\n",
    "    \"potential_attractive\": 1,\n",
    "    \"potential_repulsive\": 10,\n",
    "    \"background\": 1/10,\n",
    "    #\"object\": 1/2500,\n",
    "}\n",
    "\n",
    "# set up a model and trainer\n",
    "model = PointCloudTCN(node_indim, h_dim=8, e_dim=8, h_outdim=3, L=3, N_blocks=4, hidden_dim=100)\n",
    "model_parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "n_params = sum([np.prod(p.size()) for p in model_parameters])\n",
    "print('number trainable params:', n_params)\n",
    "trainer = TCNTrainer(model=model, loaders=loaders, loss_functions=loss_functions,\n",
    "                     lr=0.001, loss_weights=loss_weights, device=device,\n",
    "                     lr_scheduler=partial(StepLR, gamma=0.9, step_size=5))\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "705de641",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60a0cfdb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0198962",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_effs(c, h, particle_id):\n",
    "    c_id = pd.DataFrame({'c': c, 'id': particle_id})\n",
    "    clusters = c_id.groupby('c')\n",
    "    majority_pid = clusters['id'].apply(lambda x: x.mode()[0])\n",
    "    majority_counts = clusters['id'].apply(lambda x: sum(x==x.mode()[0]))\n",
    "    majority_fraction = clusters['id'].apply(lambda x: sum(x==x.mode()[0])/len(x))\n",
    "    h_id = pd.DataFrame({'hits': np.ones(len(h)), 'id': particle_id})\n",
    "    particles = h_id.groupby('id')\n",
    "    nhits = particles['hits'].apply(lambda x: len(x)).to_dict()\n",
    "    majority_hits = clusters['id'].apply(lambda x: x.mode().map(nhits)[0])\n",
    "    perfect_match = ((majority_hits==majority_counts) &\n",
    "                     (majority_fraction > 0.99))\n",
    "    double_majority = (((majority_counts / majority_hits).fillna(0) > 0.5) &\n",
    "                       (majority_fraction > 0.5))\n",
    "    lhc_match = ((majority_fraction).fillna(0) > 0.75)\n",
    "    return {\n",
    "        'total': len(np.unique(c)),\n",
    "        'perfect': sum(perfect_match),\n",
    "        'double_majority': sum(double_majority),\n",
    "        'lhc': sum(lhc_match),\n",
    "    }\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "h = np.array([[0.1, 0.2], [0.2, 0.1], [0.3, 0.1], [0.2, 0.2,], [0.3, 0.3],\n",
    "              [0.4, 0.5], [0.5, 0.6], [0.6, 0.6]])\n",
    "plt.plot(h[:,0], h[:,1], '.')\n",
    "particle_id = [0,0,0,0,0,1,1,1]\n",
    "c = [-1,-1, 0, 0, 0, 1 , 1, 2]\n",
    "get_effs(c, h, particle_id)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d99ac1ce",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
